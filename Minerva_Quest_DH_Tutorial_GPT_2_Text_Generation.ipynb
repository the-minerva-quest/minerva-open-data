{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Minerva Quest - DH Tutorial: GPT-2 Text Generation",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/the-minerva-quest/minerva-open-data/blob/main/Minerva_Quest_DH_Tutorial_GPT_2_Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri26E8HfLFBC"
      },
      "source": [
        "# !!! MAKE A COPY OF THIS NOTEBOOK FIRST !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_"
      },
      "source": [
        "#  Digital Humanities Tutorial: Generating Text with GPT-2\n",
        "The purpose of this notebook is to provide an accessible demo for generating custom-style texts with the GPT-2 architecture.To learn more about GPT-2, you should check out OpenAI's [original paper](https://openai.com/blog/better-language-models/) with amazing output samples.\n",
        "\n",
        "Please follow the instructions provided below, and if you get stuck reach out to me at armin.hamp@minerva.kgi.edu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMAQS8XU_chW"
      },
      "source": [
        "## Step I: Loading the model\n",
        "\n",
        "By running the following cells, we will load the requisite libraries and a small version of the pretrained GPT-2 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b148587b-4e2f-487b-8dbe-07d2f24312f9"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06935ce4-1729-4f7e-8cd8-2d770e521409"
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 287Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 124Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 387Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:03, 140Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 365Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 195Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 187Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPFnbi76FtEM"
      },
      "source": [
        "Running this cell will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b81fc76c-636c-4892-8fc8-98b125520cce"
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRcLaV5L_u1s"
      },
      "source": [
        "## Step II: Training your model\n",
        "\n",
        "Here you will get a chance to finetune the pre-loaded GPT-2 model with the textual style of your choice. To do this, you will have to find a corpus of text that represents a particular style that you want to reproduce. For example, this could be a collection of children's stories, romantic novels, corporate email messaging.\n",
        "\n",
        "Here are two great resources to find inspiration, but feel free to compile your own dataset.\n",
        "\n",
        "\n",
        "\n",
        "*   https://research.fb.com/downloads/babi/\n",
        "*   https://github.com/niderhoff/nlp-datasets\n",
        "\n",
        "Once found a style or genre of your liking:\n",
        "\n",
        "1. Obtain a plaintext (.txt) file of the corpus of your choice. \n",
        "2. Make sure that your file is **no larger than 50MB **for more might make the Colab Notebook run out of memory during the finetuning stage.\n",
        "3. Upload your .txt file into the sample_data folder on the left-hand side bar. See image below for reference:\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vnqs-UkfeIXJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "597be371-c3ed-4dfb-c2d8-ad7f579e185a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dn8-C1lLIauI"
      },
      "source": [
        "4. Change the filename in the cell below to the name of the plaintext you just uploaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll"
      },
      "source": [
        "#the file here is from Facebook Research's https://research.fb.com/downloads/babi/ bAbi - automatic text comprehension \n",
        "#it is a collection of children's stories\n",
        "file_name = \"quest.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS"
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3"
      },
      "source": [
        "Finetune the model with following hyperparameters. This Process should take about 20 minutes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "382b4986-efc3-43c9-b84f-a999dd19a54f"
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='run1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.56s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 439961 tokens\n",
            "Training...\n",
            "[10 | 28.16] loss=3.33 avg=3.33\n",
            "[20 | 49.58] loss=3.32 avg=3.32\n",
            "[30 | 71.20] loss=3.24 avg=3.30\n",
            "[40 | 93.02] loss=3.17 avg=3.27\n",
            "[50 | 115.02] loss=3.10 avg=3.23\n",
            "[60 | 137.19] loss=3.21 avg=3.23\n",
            "[70 | 159.60] loss=3.23 avg=3.23\n",
            "[80 | 182.12] loss=3.31 avg=3.24\n",
            "[90 | 204.77] loss=2.94 avg=3.20\n",
            "[100 | 227.56] loss=3.09 avg=3.19\n",
            "[110 | 250.54] loss=2.67 avg=3.14\n",
            "[120 | 273.51] loss=3.07 avg=3.13\n",
            "[130 | 296.56] loss=2.75 avg=3.10\n",
            "[140 | 319.65] loss=2.86 avg=3.08\n",
            "[150 | 342.75] loss=2.71 avg=3.06\n",
            "[160 | 365.83] loss=2.93 avg=3.05\n",
            "[170 | 389.00] loss=2.64 avg=3.02\n",
            "[180 | 412.32] loss=2.64 avg=3.00\n",
            "[190 | 435.64] loss=2.46 avg=2.97\n",
            "[200 | 458.93] loss=2.40 avg=2.94\n",
            "======== SAMPLE 1 ========\n",
            "                                                                                                                                                                                                                                                                                                                       the last place you will see        the last place you will see   the last place you will see      the last place you will see     the last place you will see   the last place you will see    the last place you will see    the last place you will see     the last place you will see    the last place you will see    the last place you will see     you will see this place \n",
            "  the last one you will see    the one that you will never see  n a thing you will do  n   what you will do  n  what you will not do  n  your body will always have  n the  your   the  your body will always have    your      the  your body will always    your body will always    all your       you will find your place      you will     you will     you will     you     you will    you    you      you    you are here   you will be here   you will be here   you will be here   you won   you will    you won    you will    you won       you will   your     your     you will  your    you will     you will       you will  your  your    your   your    your   your   your    your   your   your  the   everyone that you will   the  the  everyone that you will   the   the people that you will  to the   you   your  the  the  the whole  the whole  the  the whole  the whole  the whole  the whole  the whole  the whole  the  the whole  the  the whole  the whole  the  the whole  the whole  the whole  the whole  the whole  the whole   the whole  all  all  all  all  all  all  all  the whole  the whole  all  the whole  the whole  the whole  the whole  the whole  the  the whole n the n the n the  you will never   you will never  you will never  you will never  n the n the n the n and all  the whole  the whole  the whole n the n the n the n the n the entire n the whole  the whole n the  the whole your  the n the n the n the n the n  the whole your  the whole  their  the whole the whole   the entire your  the whole the whole  the whole  the whole  the whole the whole the  the whole   the whole the whole  the whole  the entire the all the whole  the whole the whole the whole n the  the whole  you          the   the   all the whole  the whole the   your  the whole the whole  the whole  the  the whole  the whole  the whole \n",
            "\n",
            "[210 | 493.43] loss=2.82 avg=2.93\n",
            "[220 | 516.74] loss=2.85 avg=2.93\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp"
      },
      "source": [
        "## Generating Text From Our Newly Trained Model\n",
        "\n",
        "The `gpt.generate()` function will now generate text from our finetuned model. Play around with the generator and see what kind of results you can get."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVySGVSBIyvp"
      },
      "source": [
        "By default, the gpt2.generate() function will generate as much text as possible (1,024 tokens) with a little bit of randomness. You can tweak the generator using the below parameters. You can also increase the temperature to increase “creativity” by allowing the network to more likely make suboptimal predictions, provide a prefix to specify how exactly you want your text to begin.\n",
        "\n",
        "As a bonus, you can bulk-generate text with gpt-2-simple by setting nsamples (number of texts to generate total) and batch_size (number of texts to generate at a time); the Colaboratory GPUs can support a batch_size of up to 20, and you can generate these to a text file with gpt2.generate_to_file(file_name) with the same parameters as gpt2.generate(). You can download the generated file locally via the sidebar, and use those to easily save and share the generated texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj42TflTmR3I"
      },
      "source": [
        "gpt2.generate(sess,\n",
        "              length=300,\n",
        "              temperature=1,\n",
        "              prefix=\"Ben Nelson, the founder and CEO of the Minerva Project, revealed himself to be a robot yesterday to a.\",\n",
        "              nsamples=2,\n",
        "              batch_size=2\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oOhiG7kMKb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InMwcFzgMtB3"
      },
      "source": [
        "Ben Nelson, the founder and CEO of the Minerva Project, revealed himself to be a robot yesterday. Nelson claimed, incorrectly, that he had created the self-balancing robot at his side, and that he has since worked to figure out how it works.  Other claims have him claiming to have been playing a game he created called “Defense Against The Damned.” In this game, an opponent who is clearly hurt or dead can be saved with a pivotal critical decision. However, it is unlikely that Nelson’s self-balancing self-balancing system will live up to this status. It would take active action to stop it, and only a small minority of successful robot defense mechanisms exist. Furthermore, even a small minority is never enough. In order to operate the self-balancing system, a robot has to be very, very safe, and have a significant amount of fatalities at the end.  Another obstacle may be determining which safe and reliable methods of defibrillator installation are more efficient. Since 2002, researchers at Lawrence Berkeley National Laboratory (BerkeleyNLL) have been working to develop better ways to detect when a self-balancing device is disengaging. These advancements have caused concerns among researchers that the devices could make it harder to defibrillate service members first, and that defibrillators may become a de facto hand-me-down approach.  The self-balancing claims are unfounded. Self-balancing claims vary depending on a customer’s age, dependency, and/or geographic area. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mywCZOpXOrA3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}